#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIèµ„è®¯æ—¥æŠ¥è‡ªåŠ¨åŒ–ç³»ç»Ÿ - ä¸»ç¨‹åº
åè°ƒå„æ¨¡å—æ‰§è¡Œå®Œæ•´å·¥ä½œæµ

è®¾è®¡è¦ç‚¹ï¼š
- ä¼˜å…ˆä» ai-hourly-buzz çš„å…±äº«æ•°æ®è¯»å–å·²é‡‡é›†æ–°é—»
- å›é€€åˆ°ç‹¬ç«‹ RSS é‡‡é›†
- æ·±åº¦å¤„ç†ï¼šå…³é”®è¯ç­›é€‰ â†’ å»é‡ â†’ æ­£æ–‡æå– â†’ AIæ‘˜è¦ç¿»è¯‘ â†’ 5ç±»åˆ†ç±»
- åŒè¾“å‡ºï¼šHTMLå­˜æ¡£ + å¾®ä¿¡å…¬ä¼—å·è‰ç¨¿
"""

import logging
import sys
import json
import hashlib
from pathlib import Path
from datetime import datetime
from typing import List

PROJECT_ROOT = Path(__file__).parent
sys.path.insert(0, str(PROJECT_ROOT))

from config.settings import (
    LOG_LEVEL, LOG_FORMAT, LOG_FILE, LOGS_DIR,
    MAX_NEWS_PER_CATEGORY
)
from crawler.models import RawNewsItem, ScoredNewsItem
from crawler.shared_loader import SharedDataLoader
from crawler.rss_parser import RSSParser
from crawler.web_scraper import WebScraper
from crawler.content_extractor import ContentExtractor
from processor.filter import KeywordFilter
from processor.deduplicator import Deduplicator
from processor.time_handler import TimeHandler
from ai_service.summarizer import Summarizer
from ai_service.translator import Translator
from ai_service.classifier import Classifier
from ai_service.deepseek_client import get_client
from publisher.html_generator import HTMLGenerator
from publisher.markdown_generator import MarkdownGenerator
from publisher.wechat_publisher import WeChatPublisher


def setup_logging():
    """é…ç½®æ—¥å¿—"""
    LOGS_DIR.mkdir(parents=True, exist_ok=True)

    handlers = [
        logging.StreamHandler(sys.stdout),
        logging.FileHandler(LOG_FILE, encoding='utf-8')
    ]

    logging.basicConfig(
        level=getattr(logging, LOG_LEVEL),
        format=LOG_FORMAT,
        handlers=handlers
    )

    logging.getLogger("urllib3").setLevel(logging.WARNING)
    logging.getLogger("requests").setLevel(logging.WARNING)
    logging.getLogger("feedparser").setLevel(logging.WARNING)


class DailyReportPipeline:
    """æ—¥æŠ¥ç”Ÿæˆæµæ°´çº¿"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)

        # æ•°æ®é‡‡é›†
        from config.rss_sources import RSS_SOURCES
        from config.settings import REQUEST_HEADERS, REQUEST_TIMEOUT, REQUEST_DELAY
        self.shared_loader = SharedDataLoader()
        self.rss_parser = RSSParser(sources=RSS_SOURCES, headers=REQUEST_HEADERS, timeout=REQUEST_TIMEOUT, delay=REQUEST_DELAY)
        self.web_scraper = WebScraper(sources=RSS_SOURCES, headers=REQUEST_HEADERS, timeout=REQUEST_TIMEOUT, delay=REQUEST_DELAY)
        self.content_extractor = ContentExtractor(headers=REQUEST_HEADERS)

        # æ•°æ®å¤„ç†
        self.keyword_filter = KeywordFilter()
        self.deduplicator = Deduplicator()
        self.time_handler = TimeHandler()

        # AIæœåŠ¡
        self.summarizer = Summarizer()
        self.translator = Translator()
        self.classifier = Classifier()

        # å‘å¸ƒ
        self.html_generator = HTMLGenerator()
        self.markdown_generator = MarkdownGenerator()
        self.wechat_publisher = WeChatPublisher()

    def run(self, publish_to_wechat: bool = True) -> bool:
        """æ‰§è¡Œå®Œæ•´çš„æ—¥æŠ¥ç”Ÿæˆæµç¨‹"""
        start_time = datetime.now()
        self.logger.info("=" * 50)
        self.logger.info("å¼€å§‹ç”ŸæˆAIèµ„è®¯æ—¥æŠ¥")
        self.logger.info("=" * 50)

        try:
            # 1. é‡‡é›†æ–°é—»ï¼ˆä¼˜å…ˆå…±äº«æ•°æ®ï¼‰
            self.logger.info("\nğŸ“¥ æ­¥éª¤1: é‡‡é›†æ–°é—»...")
            raw_news = self._collect_news()
            if not raw_news:
                self.logger.warning("æœªè·å–åˆ°ä»»ä½•æ–°é—»ï¼Œæµç¨‹ç»ˆæ­¢")
                return False

            # 2. æ—¶é—´è¿‡æ»¤
            self.logger.info("\nğŸ“… æ­¥éª¤2: ç­›é€‰è¿‡å»24å°æ—¶æ–°é—»...")
            recent_news = self._filter_by_time(raw_news)
            if not recent_news:
                self.logger.warning("æœªæ‰¾åˆ°è¿‡å»24å°æ—¶çš„æ–°é—»ï¼Œä½¿ç”¨æ‰€æœ‰æ–°é—»")
                recent_news = raw_news[:50]

            # 3. å…³é”®è¯ç­›é€‰
            self.logger.info("\nğŸ” æ­¥éª¤3: ç­›é€‰ç›¸å…³æ–°é—»...")
            filtered_news = self._filter_news(recent_news)
            if not filtered_news:
                self.logger.warning("ç­›é€‰åæ— ç›¸å…³æ–°é—»ï¼Œæµç¨‹ç»ˆæ­¢")
                return False

            # 4. å»é‡
            self.logger.info("\nğŸ”„ æ­¥éª¤4: å»é‡å¤„ç†...")
            unique_news = self._deduplicate(filtered_news)
            if not unique_news:
                self.logger.warning("å»é‡åæ— æ–°é—»ï¼Œæµç¨‹ç»ˆæ­¢")
                return False

            # 4.5 æŒ‰è¯„åˆ†æ’åºï¼Œå–Top N
            unique_news.sort(key=lambda x: x.relevance_score, reverse=True)
            MAX_TOTAL = 50  # æœ€å¤šå¤„ç†50æ¡é«˜è´¨é‡æ–°é—»
            if len(unique_news) > MAX_TOTAL:
                self.logger.info(f"æŒ‰è¯„åˆ†å–å‰ {MAX_TOTAL} æ¡ (å…± {len(unique_news)} æ¡)")
                unique_news = unique_news[:MAX_TOTAL]

            # æ‰“å°Top10æ ‡é¢˜å’Œè¯„åˆ†ä¾›è°ƒè¯•
            self.logger.info("Top-10 æ–°é—»:")
            for i, item in enumerate(unique_news[:10]):
                title = item.raw_item.title[:60]
                self.logger.info(f"  {i+1}. [{item.relevance_score:.1f}åˆ†] {title}")

            # 5. æå–æ­£æ–‡
            self.logger.info("\nğŸ“„ æ­¥éª¤5: æå–æ–°é—»æ­£æ–‡...")
            self._extract_content(unique_news)

            # 6. AIå¤„ç†ï¼ˆæ‘˜è¦+ç¿»è¯‘ï¼‰
            self.logger.info("\nğŸ¤– æ­¥éª¤6: AIç”Ÿæˆæ‘˜è¦å’Œç¿»è¯‘...")
            processed_news = self._ai_process(unique_news)

            # 7. åˆ†ç±»ï¼ˆäº”ä¸ªç±»åˆ«ï¼‰
            self.logger.info("\nğŸ“Š æ­¥éª¤7: æ–°é—»åˆ†ç±»...")
            categorized_news = self._classify_news(processed_news)

            # 8. é™åˆ¶æ¯ä¸ªç±»åˆ«çš„æ•°é‡
            for category in categorized_news:
                categorized_news[category] = categorized_news[category][:MAX_NEWS_PER_CATEGORY]

            total_count = sum(len(items) for items in categorized_news.values())

            # 9. ç”ŸæˆHTMLå’ŒMarkdown
            self.logger.info("\nğŸ“ æ­¥éª¤8: ç”Ÿæˆæ—¥æŠ¥...")
            daily_summary = self._generate_daily_summary(categorized_news)
            html_content = self.html_generator.generate(categorized_news, daily_summary)

            token_usage = get_client().get_total_tokens()
            self.markdown_generator.generate(categorized_news, daily_summary, token_usage)

            # 10. å‘å¸ƒåˆ°å¾®ä¿¡
            if publish_to_wechat:
                self.logger.info("\nğŸ“¤ æ­¥éª¤9: å‘å¸ƒåˆ°å¾®ä¿¡å…¬ä¼—å·...")
                self._publish_to_wechat(html_content)

            # ç»Ÿè®¡
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()

            self.logger.info("\n" + "=" * 50)
            self.logger.info("âœ… æ—¥æŠ¥ç”Ÿæˆå®Œæˆ!")
            from ai_service.classifier import CATEGORY_DEFINITIONS
            for cat_key, cat_items in categorized_news.items():
                cat_name = CATEGORY_DEFINITIONS[cat_key]["name"]
                self.logger.info(f"   - {cat_name}: {len(cat_items)} æ¡")
            self.logger.info(f"   - æ€»è®¡: {total_count} æ¡")
            self.logger.info(f"   - è€—æ—¶: {duration:.1f} ç§’")
            self.logger.info(f"   - Tokenæ¶ˆè€—: {get_client().get_total_tokens()}")
            self.logger.info("=" * 50)

            return True

        except Exception as e:
            self.logger.error(f"æ—¥æŠ¥ç”Ÿæˆå¤±è´¥: {e}", exc_info=True)
            return False

    def _collect_news(self) -> List[RawNewsItem]:
        """é‡‡é›†æ–°é—» â€” ä¼˜å…ˆä» ai-hourly-buzz å…±äº«æ•°æ®è¯»å–"""
        all_news = []

        # å°è¯•åŠ è½½å…±äº«æ•°æ®
        self.logger.info("å°è¯•ä» ai-hourly-buzz å…±äº«æ•°æ®åŠ è½½...")
        shared_news = self.shared_loader.load()
        if shared_news:
            all_news.extend(shared_news)
            self.logger.info(f"å…±äº«æ•°æ®è·å–: {len(shared_news)} æ¡")

        # å¦‚æœå…±äº«æ•°æ®ä¸è¶³ï¼Œç‹¬ç«‹é‡‡é›†
        if len(all_news) < 10:
            self.logger.info("å…±äº«æ•°æ®ä¸è¶³ï¼Œå¯åŠ¨ç‹¬ç«‹é‡‡é›†...")

            self.logger.info("ä»RSSæºé‡‡é›†...")
            rss_news = self.rss_parser.parse_all()
            all_news.extend(rss_news)
            self.logger.info(f"RSSæºè·å–: {len(rss_news)} æ¡")

            self.logger.info("ä»ç½‘é¡µé‡‡é›†...")
            web_news = self.web_scraper.scrape_all()
            all_news.extend(web_news)
            self.logger.info(f"ç½‘é¡µçˆ¬å–: {len(web_news)} æ¡")

        self.logger.info(f"å…±é‡‡é›†: {len(all_news)} æ¡æ–°é—»")
        return all_news

    def _filter_by_time(self, news_list: List[RawNewsItem]) -> List[RawNewsItem]:
        """æŒ‰æ—¶é—´ç­›é€‰ï¼ˆè¿‡å»24å°æ—¶ï¼‰"""
        start, end = self.time_handler.get_24h_range()
        self.logger.info(f"ç­›é€‰æ—¶é—´èŒƒå›´: {start.strftime('%Y-%m-%d %H:%M')} ~ {end.strftime('%Y-%m-%d %H:%M')}")

        filtered = []
        no_time_count = 0

        for item in news_list:
            if item.pub_time is None:
                filtered.append(item)
                no_time_count += 1
                continue

            beijing_time = self.time_handler.convert_to_beijing(item.pub_time)
            item.pub_time = beijing_time

            if start <= beijing_time <= end:
                filtered.append(item)

        self.logger.info(f"æ—¶é—´è¿‡æ»¤: {len(news_list)} -> {len(filtered)} æ¡ (æ— æ—¶é—´æˆ³: {no_time_count})")
        return filtered

    def _filter_news(self, news_list: List[RawNewsItem]) -> List[ScoredNewsItem]:
        """å…³é”®è¯ç­›é€‰"""
        return self.keyword_filter.filter_news(news_list)

    def _deduplicate(self, news_list: List[ScoredNewsItem]) -> List[ScoredNewsItem]:
        return self.deduplicator.deduplicate(news_list)

    def _extract_content(self, news_list: List[ScoredNewsItem]):
        """æå–æ­£æ–‡"""
        items_to_extract = []
        for item in news_list:
            if not item.raw_item.content or len(item.raw_item.content) < 100:
                items_to_extract.append(item.raw_item)

        if items_to_extract:
            self.logger.info(f"éœ€è¦æå–æ­£æ–‡: {len(items_to_extract)} æ¡")
            self.content_extractor.extract_batch(items_to_extract)

    def _ai_process(self, news_list: List[ScoredNewsItem]) -> List[ScoredNewsItem]:
        """AIå¤„ç†ï¼šç”Ÿæˆæ‘˜è¦å’Œç¿»è¯‘æ ‡é¢˜"""
        from ai_service.summarizer import Summarizer
        news_list = self.summarizer.summarize_batch(news_list)

        # è¿‡æ»¤æ‰æ— æ•ˆæ‘˜è¦çš„æ–°é—»ï¼ˆæ¨¡å‹å£°ç§°"å†…å®¹ä¸ºç©º/ç¼ºå¤±"ç­‰ï¼‰
        before_count = len(news_list)
        news_list = [item for item in news_list if not Summarizer.is_invalid_summary(item.summary_cn)]
        filtered = before_count - len(news_list)
        if filtered:
            self.logger.info(f"è¿‡æ»¤æ‰ {filtered} æ¡æ— æ•ˆæ‘˜è¦çš„æ–°é—»ï¼Œå‰©ä½™ {len(news_list)} æ¡")

        # æ”¶é›†æ‰€æœ‰éœ€è¦ç¿»è¯‘çš„è‹±æ–‡æ ‡é¢˜ï¼ˆåŸºäºå®é™…å†…å®¹æ£€æµ‹ï¼Œè€Œé language å­—æ®µï¼‰
        en_indices = []
        en_titles = []
        for i, item in enumerate(news_list):
            title = item.raw_item.title
            cn_ratio = sum(1 for c in title if '\u4e00' <= c <= '\u9fff') / max(len(title), 1)
            if cn_ratio < 0.3:  # ä¸­æ–‡å­—ç¬¦ä¸è¶³30%ï¼Œè§†ä¸ºè‹±æ–‡æ ‡é¢˜éœ€ç¿»è¯‘
                en_indices.append(i)
                en_titles.append(title)
            else:
                # ä¸­æ–‡æ ‡é¢˜ç›´æ¥è®¾ç½®
                item.title_cn = title

        if en_titles:
            self.logger.info(f"æ‰¹é‡ç¿»è¯‘ {len(en_titles)} ä¸ªè‹±æ–‡æ ‡é¢˜...")
            cn_titles = self.translator.translate_batch_titles(en_titles)
            if cn_titles and len(cn_titles) == len(en_titles):
                for i, idx in enumerate(en_indices):
                    if cn_titles[i]:
                        news_list[idx].title_cn = cn_titles[i]

        # å…œåº•ï¼šå¯¹ä»æ— ä¸­æ–‡æ ‡é¢˜çš„è‹±æ–‡æ–°é—»é€æ¡ç¿»è¯‘
        untranslated = 0
        for item in news_list:
            if not item.title_cn:
                title = item.raw_item.title
                cn_ratio = sum(1 for c in title if '\u4e00' <= c <= '\u9fff') / max(len(title), 1)
                if cn_ratio < 0.3:
                    translated = self.translator.translate_title(title)
                    if translated:
                        item.title_cn = translated
                        untranslated += 1
                    else:
                        item.title_cn = title
                else:
                    item.title_cn = title
        if untranslated:
            self.logger.info(f"é€æ¡ç¿»è¯‘å…œåº•: {untranslated} ä¸ªæ ‡é¢˜")

        return news_list

    def _classify_news(self, news_list: List[ScoredNewsItem]) -> dict:
        return self.classifier.classify_batch(news_list, use_ai=False)

    def _generate_daily_summary(self, categorized_news: dict) -> str:
        """ä½¿ç”¨AIç”Ÿæˆæ¯æ—¥å¯¼è¯­"""
        titles = []
        for category, items in categorized_news.items():
            for item in items[:2]:
                title = item.title_cn or item.raw_item.title
                titles.append(title[:50])

        if not titles:
            return "ä»Šæ—¥AIè¡Œä¸šæš‚æ— é‡å¤§åŠ¨æ€æ›´æ–°ã€‚"

        titles = titles[:8]

        prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹ä»Šæ—¥AIèµ„è®¯æ ‡é¢˜ï¼Œç”Ÿæˆä¸€æ®µ50-80å­—çš„æ¯æ—¥å¯¼è¯­æ‘˜è¦ï¼Œæ¦‚æ‹¬ä»Šæ—¥AIé¢†åŸŸçš„ä¸»è¦åŠ¨æ€ï¼š

{chr(10).join(['- ' + t for t in titles])}

è¦æ±‚ï¼š
1. ç®€æ´æ¦‚æ‹¬ä»Šæ—¥ä¸»è¦åŠ¨æ€
2. çªå‡ºé‡ç‚¹å…¬å¸å’ŒæŠ€æœ¯
3. è¯­è¨€æµç•…ï¼Œé€‚åˆä½œä¸ºæ—¥æŠ¥å¼€å¤´
4. ç›´æ¥è¾“å‡ºå¯¼è¯­å†…å®¹ï¼Œä¸è¦åŠ ä»»ä½•å‰ç¼€"""

        try:
            client = get_client()
            response = client.chat([
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç§‘æŠ€æ–°é—»ç¼–è¾‘ã€‚"},
                {"role": "user", "content": prompt}
            ], temperature=0.5, max_tokens=200)

            if response:
                return response.strip().strip('"\'')
        except Exception as e:
            self.logger.warning(f"ç”Ÿæˆå¯¼è¯­å¤±è´¥: {e}")

        total_count = sum(len(items) for items in categorized_news.values())
        return f"ä»Šæ—¥AIé¢†åŸŸå…±æœ‰{total_count}æ¡åŠ¨æ€å€¼å¾—å…³æ³¨ã€‚"

    def _publish_to_wechat(self, html_content: str) -> bool:
        return self.wechat_publisher.publish_daily_report(html_content)


def main():
    """ä¸»å‡½æ•°"""
    setup_logging()

    publish_to_wechat = True
    if "--no-publish" in sys.argv or "--local-only" in sys.argv:
        publish_to_wechat = False
        logging.info("ä»…ç”Ÿæˆæœ¬åœ°æ–‡ä»¶ï¼Œä¸å‘å¸ƒåˆ°å¾®ä¿¡")

    pipeline = DailyReportPipeline()
    success = pipeline.run(publish_to_wechat=publish_to_wechat)

    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
